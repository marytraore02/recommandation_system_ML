import random
import uuid
from contextlib import asynccontextmanager
from datetime import datetime
from decimal import Decimal
import asyncio
from collections import defaultdict
import logging
import json
from dotenv import load_dotenv
import os
from typing import List, Dict, Any, Optional, Set
from fastapi import FastAPI, HTTPException, Query, status, Depends, APIRouter, Header, Path, Request
from typing import Annotated
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from pydentics import models
from pydentics import schemas2
from pydentics import schemas3

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import joinedload, Session
from sqlalchemy.orm import selectinload, aliased
from sqlalchemy import select, func, union_all, literal_column, join
from sqlalchemy.dialects.postgresql import UUID, array
from sqlalchemy import select, func, and_
import sqlalchemy as sa
from sqlalchemy import case, cast, TEXT

from aiokafka import AIOKafkaProducer
from pydantic import BaseModel, Field
import redis
import redis.asyncio as aioredis

from db.redis_client import get_redis_client, redis_connection_pool
from db.postgres_config import get_db, AsyncSession, engine, create_db_and_tables
from utils.auxiliaires_fonctions import build_feed_response, _get_popular_ids_from_redis, _get_trending_data_from_redis, _get_new_arrivals_ids_from_db, enrich_feed_with_presigned_urls, check_cache_freshness, select_feed_items
from worker.feed.feed_popular_trending_new import schedule_feed_generation, generate_and_cache_feed_for_country

from utils import config


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS")
KAFKA_TOPIC_USER_EVENT = os.getenv("KAFKA_TOPIC_USER_EVENT")
SPRING_BOOT_API_URL = os.getenv("SPRING_BOOT_API_URL")


# ===================================================================
#OPTIMISATION 1: G√©rer le producteur Kafka via le cycle de vie de l'app
# ===================================================================
@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("üöÄ D√©marrage de l'application FastAPI...")
    # --- Initialisation ---
    # 1. Connexion √† Redis
    await redis_connection_pool.ping()
    logger.info("Connexion √† Redis r√©ussie !")

    # 2. Cr√©er le producteur Kafka et le maintenir en vie
    logger.info("D√©marrage du producteur Kafka...")
    app.state.kafka_producer = AIOKafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)
    await app.state.kafka_producer.start()
    logger.info("Producteur Kafka d√©marr√©.")
    
    yield
    
    logger.info("üõë Arr√™t de l'application...")
    # --- Nettoyage ---
    # 1. Arr√™ter le producteur Kafka
    logger.info("Arr√™t du producteur Kafka...")
    await app.state.kafka_producer.stop()
    logger.info("Producteur Kafka arr√™t√©.")

    # 2. Fermer le pool Redis
    await redis_connection_pool.close()
    logger.info("Pool de connexions Redis ferm√©.")
    
    # 3. Fermer le pool de connexions de la base de donn√©es
    await engine.dispose()
    logger.info("Pool de connexions de la base de donn√©es ferm√©.")

    
app = FastAPI(
    title="Event Ingestion API",
    description="An ultra-fast API to receive event batches and push them to Kafka.",
    lifespan=lifespan
)

#===============================Methodes==========================
# D√©pendance pour injecter le producteur Kafka dans les endpoints
async def get_kafka_producer(request: Request) -> AIOKafkaProducer:
    return request.app.state.kafka_producer
#===============================Methodes==========================



#=========================RECEVELY-EVENT-USER_ENDPOINT===========================
@app.post(
    "/events/collect",
    status_code=status.HTTP_202_ACCEPTED,
    summary="Accept a batch of user events"
)
async def accept_events(
    batch: schemas2.EventBatchModel,
    producer: AIOKafkaProducer = Depends(get_kafka_producer) # Injection de d√©pendance
):
    """
    Accepte un lot d'√©v√©nements, les envoie √† Kafka et r√©pond imm√©diatement.
    Utilise le producteur Kafka partag√© pour une performance maximale.
    """
    try:
        logger.info(f"Received a batch of {len(batch.events)} events. Sending to Kafka topic '{KAFKA_TOPIC_USER_EVENT}'.")

        tasks = []
        for event in batch.events:
            message = event.model_dump_json().encode("utf-8")
            tasks.append(producer.send(KAFKA_TOPIC_USER_EVENT, message))
        
        await asyncio.gather(*tasks)

        logger.info("Batch successfully sent to Kafka.")
        return {"status": "accepted", "message": f"{len(batch.events)} events queued."}

    except Exception as e:
        logger.error(f"Error sending to Kafka: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Could not send events to the processing queue. Please try again later."
        )

#=========================RECEVELY-EVENT-USER_ENDPOINT===========================



# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Analytiques & Recommandations
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#=========================COLD-START-ENDPOINT===================================
@app.get(
    "/cold-start/combined/feed/v1/{country}",
    response_model=List[schemas3.CombinedFeedItem],
    summary="Feed combin√© et intelligent (ultra-optimis√©)",
    description="""
    R√©cup√®re un feed pr√©-calcul√© et mis en cache pour une performance maximale.
    
    - ‚ö° Latence cible: <50ms
    - üîÑ Cache mis √† jour toutes les 2 minutes
    - üé≤ Ordre al√©atoire pour chaque requ√™te
    - üåç Pays support√©s: Mali, Cameroun, Senegal
    """,
    tags=["Feed"]
)
async def get_combined_feed_optimized(
    country: str,
    redis: aioredis.Redis = Depends(get_redis_client),
    limit: int = Query(
        config.EndpointConfig.DEFAULT_LIMIT,
        ge=config.EndpointConfig.MIN_LIMIT,
        le=config.EndpointConfig.MAX_LIMIT,
        description="Nombre d'√©l√©ments √† retourner"
    ),
    randomize: bool = Query(
        True,
        description="M√©langer les r√©sultats al√©atoirement"
    ),
    seed: Optional[int] = Query(
        None,
        description="Seed pour randomisation (tests uniquement)"
    )
):
    """
    Endpoint ultra-optimis√© pour r√©cup√©rer le feed combin√©.
    
    Performance optimizations:
    - Lecture directe depuis Redis (pas de DB)
    - Enrichissement des URLs √† la vol√©e uniquement
    - S√©lection al√©atoire apr√®s enrichissement
    - Gestion d'erreurs granulaire
    """
    
    # ===================================================================
    # √âTAPE 2: R√âCUP√âRATION DEPUIS LE CACHE
    # ===================================================================
    cache_key = config.EndpointConfig.CACHE_KEY_TEMPLATE.format(country=country)
    
    try:
        # V√©rifier la fra√Æcheur du cache
        ttl = await check_cache_freshness(redis, cache_key)
        
        if ttl is None:
            # Cache expir√© ou inexistant
            if config.EndpointConfig.ENABLE_FALLBACK:
                raise HTTPException(
                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                    detail={
                        "message": f"Le feed pour '{country}' est en cours de g√©n√©ration.",
                        "retry_after": f"{config.EndpointConfig.FALLBACK_RETRY_DELAY} secondes",
                        "country": country
                    }
                )
            else:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"Aucun feed disponible pour '{country}'"
                )
        
        # R√©cup√©rer les donn√©es du cache
        cached_data = await redis.get(cache_key)
        
        if not cached_data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Feed vide pour '{country}'"
            )
        
        # D√©s√©rialisation
        feed_items_raw = json.loads(cached_data)
        
        if not feed_items_raw:
            return []
        
        print(f"‚úÖ Cache hit pour {country}: {len(feed_items_raw)} √©l√©ments (TTL: {ttl}s)")
    
    except HTTPException:
        raise
    except json.JSONDecodeError as e:
        print(f"‚ùå Erreur de d√©s√©rialisation JSON pour {country}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Erreur de format des donn√©es en cache"
        )
    except Exception as e:
        print(f"‚ùå Erreur Redis pour {country}: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Service de cache temporairement indisponible"
        )
    
    # ===================================================================
    # √âTAPE 3: S√âLECTION DES √âL√âMENTS (AVANT ENRICHISSEMENT)
    # ===================================================================
    # OPTIMISATION: S√©lectionner AVANT d'enrichir pour limiter les appels de pr√©signature
    selected_items = select_feed_items(
        feed_items_raw,
        limit=limit,
        randomize=randomize,
        seed=seed
    )
    
    if not selected_items:
        return []
    
    # ===================================================================
    # √âTAPE 4: ENRICHISSEMENT DES URLS PR√âSIGN√âES
    # ===================================================================
    try:
        enriched_feed = await enrich_feed_with_presigned_urls(selected_items)
        
        print(f"üé¨ Feed enrichi pour {country}: {len(enriched_feed)} √©l√©ments")
        
        return enriched_feed
    
    except Exception as e:
        print(f"‚ùå Erreur lors de l'enrichissement des URLs pour {country}: {e}")
        
        # FALLBACK: Retourner les donn√©es sans URLs pr√©sign√©es
        # plut√¥t que de tout faire √©chouer
        print(f"‚ö†Ô∏è Fallback: retour des donn√©es sans URLs pr√©sign√©es")
        
        # Retourner les donn√©es brutes (sans enrichissement)
        # Attention: v√©rifier que votre mod√®le accepte des URLs vides/None
        return selected_items



# @app.get(
#     "/feed/v1/combined2/{country}",
#     response_model=List[schemas3.CombinedFeedItem],
#     summary="G√©n√®re un feed combin√© et intelligent",
#     description="M√©lange les posts populaires, tendance et les nouveaut√©s pour cr√©er un feed dynamique."
# )
# async def get_combined_feed(
#     country: str,
#     db: AsyncSession = Depends(get_db),
#     redis: aioredis.Redis = Depends(get_redis_client),
#     limit: int = Query(20, ge=5, le=100, description="Nombre total d'√©l√©ments √† retourner."),
#     popular_ratio: int = Query(3, ge=1, description="Ratio d'√©l√©ments populaires dans le mix."),
#     trending_ratio: int = Query(2, ge=1, description="Ratio d'√©l√©ments tendance dans le mix."),
#     new_arrivals_count: int = Query(2, ge=0, le=10, description="Nombre de nouveaut√©s √† injecter au d√©but du feed.")
# ):
#     #ge=5, le=100: Ce sont des contraintes de validation. La valeur doit √™tre sup√©rieure ou √©gale √† 5 (ge pour "greater or equal") et inf√©rieure ou √©gale √† 100 (le pour "less or equal").
#     """
#     Cet endpoint cr√©e un feed personnalis√© en combinant plusieurs sources :
#     1.  **Nouveaut√©s** : Les X derniers posts pour assurer la visibilit√© des nouveaux contenus.
#     2.  **Populaires** : Les posts les plus vus/lik√©s.
#     3.  **Tendance** : Les posts avec une forte v√©locit√© de popularit√©.

#     Le r√©sultat est un m√©lange intelligent pour maximiser l'engagement utilisateur.
#     """
#     # ===================================================================
#     # √âTAPE 1: R√âCUP√âRATION PARALL√àLE DES DONN√âES SOURCES
#     # ===================================================================
#     try:
#         popular_ids_task = _get_popular_ids_from_redis(redis, country)
#         trending_data_task = _get_trending_data_from_redis(redis, country)
#         new_arrivals_ids_task = _get_new_arrivals_ids_from_db(db, new_arrivals_count)

#         popular_ids, trending_videos_raw, new_arrivals_ids = await asyncio.gather(
#             popular_ids_task, trending_data_task, new_arrivals_ids_task
#         )
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es sources: {e}")

#     # ===================================================================
#     # √âTAPE 2: TRAITEMENT & FILTRAGE DES DONN√âES TENDANCE
#     # ===================================================================
#     # On applique les filtres business sur les donn√©es tendance (comme dans l'ancien endpoint)
#     # Pour cet exemple, on trie simplement par score
#     trending_videos_up_only = [
#         v for v in trending_videos_raw if v.get('trend_direction') == 'up'
#     ]
    
#     # On utilise la liste filtr√©e pour la suite
#     trending_ids = [int(v['video_id']) for v in trending_videos_up_only]
#     trending_data_map = {int(v['video_id']): v for v in trending_videos_up_only}


#     # ===================================================================
#     # √âTAPE 3: M√âLANGE INTELLIGENT ET D√âDOUBLONNAGE
#     # ===================================================================
#     final_ordered_ids = []
#     reasons_map = {}
#     seen_ids = set()

#     # 3.1 - Injecter les nouveaut√©s en priorit√©
#     for video_id in new_arrivals_ids:
#         if video_id not in seen_ids:
#             final_ordered_ids.append(video_id)
#             reasons_map[video_id] = schemas3.CombinedFeedReason.NEW_ARRIVAL
#             seen_ids.add(video_id)

#     # 3.2 - M√©langer populaires et tendance selon le ratio
#     popular_iter = iter(popular_ids)
#     trending_iter = iter(trending_ids)
    
#     while len(final_ordered_ids) < limit:
#         added_in_cycle = 0
#         # Ajouter des posts populaires
#         for _ in range(popular_ratio):
#             try:
#                 video_id = next(popular_iter)
#                 if video_id not in seen_ids:
#                     final_ordered_ids.append(video_id)
#                     reasons_map[video_id] = schemas3.CombinedFeedReason.POPULAR
#                     seen_ids.add(video_id)
#                     added_in_cycle += 1
#                     if len(final_ordered_ids) >= limit: break
#             except StopIteration:
#                 break # Plus de posts populaires

#         if len(final_ordered_ids) >= limit: break

#         # Ajouter des posts tendance
#         for _ in range(trending_ratio):
#             try:
#                 video_id = next(trending_iter)
#                 if video_id not in seen_ids:
#                     final_ordered_ids.append(video_id)
#                     reasons_map[video_id] = schemas3.CombinedFeedReason.TRENDING
#                     seen_ids.add(video_id)
#                     added_in_cycle += 1
#                     if len(final_ordered_ids) >= limit: break
#             except StopIteration:
#                 break # Plus de posts tendance

#         if added_in_cycle == 0:
#             break # Si on n'a plus rien √† ajouter, on sort

#     if not final_ordered_ids:
#         return []

#     # ===================================================================
#     # √âTAPE 4: ENRICHISSEMENT UNIFI√â VIA LA BASE DE DONN√âES
#     # ===================================================================
#     Post = models.CagnottePostModel
#     Ressource = models.RessourceModel

#     # Map pour pr√©server l'ordre du m√©lange
#     order_map = {video_id: index for index, video_id in enumerate(final_ordered_ids)}
#     order_logic = case(order_map, value=Ressource.id)

#     # Requ√™te unique pour r√©cup√©rer tous les posts n√©cessaires
#     stmt_posts = (
#         select(Post)
#         .options(
#             selectinload(Post.cagnotte).selectinload(models.CagnotteModel.categorie),
#             selectinload(Post.cagnotte).selectinload(models.CagnotteModel.admin),
#             selectinload(Post.author)
#         )
#         .join(Ressource, Post.id == Ressource.reference)
#         .where(Ressource.id.in_(final_ordered_ids))
#         .order_by(order_logic)
#     )
#     result_posts = await db.execute(stmt_posts)
#     # .unique() est crucial car un post peut matcher plusieurs ressources (vid√©os)
#     relevant_posts = result_posts.scalars().unique().all()
    
#     post_ids = [p.id for p in relevant_posts]
    
#     # Requ√™te pour r√©cup√©rer TOUTES les ressources de ces posts
#     stmt_resources = select(Ressource).where(Ressource.reference.in_(post_ids)).order_by(Ressource.order_index.asc())
#     result_resources = await db.execute(stmt_resources)
#     resources_by_post_id = defaultdict(list)
#     for res in result_resources.scalars().all():
#         resources_by_post_id[res.reference].append(res)

#     # ===================================================================
#     # √âTAPE 5: ASSEMBLAGE DE LA R√âPONSE FINALE
#     # ===================================================================
#     final_response_models = []
#     for post in relevant_posts:
#         ressources = resources_by_post_id.get(post.id, [])
        
#         # Trouver quel ID de ressource a mis ce post dans notre liste
#         # C'est un peu complexe, mais n√©cessaire si un post a plusieurs vid√©os
#         trigger_ressource_id = None
#         for r in ressources:
#             if r.id in reasons_map:
#                 trigger_ressource_id = r.id
#                 break
        
#         if trigger_ressource_id is None:
#             continue # Ne devrait pas arriver, mais s√©curit√©

#         # Construire l'objet de base
#         base_feed_item = build_feed_response(post, ressources)

#         # Cr√©er l'objet final enrichi
#         reason = reasons_map[trigger_ressource_id]
        
#         # Cr√©er un dictionnaire de donn√©es pour l'objet final
#         response_data = base_feed_item.model_dump()
#         response_data['reason'] = reason

#         if reason == schemas3.CombinedFeedReason.TRENDING:
#             trending_info = trending_data_map.get(trigger_ressource_id, {})
#             response_data['trendingScore'] = trending_info.get('trending_score')
#             response_data['velocityPercent'] = trending_info.get('velocity_percent')
#             response_data['trendDirection'] = trending_info.get('trend_direction')
        
#         final_response_models.append(schemas3.CombinedFeedItem.model_validate(response_data))

#     # ===================================================================
#     # NOUVELLE √âTAPE 6: ENRICHISSEMENT AVEC LES URLS PR√âSIGN√âES
#     # ===================================================================
#     if not final_response_models:
#         return []

#     # On convertit nos mod√®les Pydantic en dictionnaires pour pouvoir les modifier
#     feed_items_as_dicts = [item.model_dump() for item in final_response_models]

#     # On appelle notre nouvelle fonction qui va tout convertir en parall√®le
#     enriched_feed = await enrich_feed_with_presigned_urls(feed_items_as_dicts)
    
#     # ===================================================================
#     # √âTAPE 7: FINALISATION
#     # ===================================================================
#     random.shuffle(enriched_feed)
    
#     # FastAPI s'occupera de valider que la liste de dictionnaires correspond au `response_model`
#     return enriched_feed

#=========================COLD-START-ENDPOINT===================================


#===============================Preferences==========================

@app.get(
    "/users/preferences/categories/{user_id}",
    response_model=List[schemas2.Categorie],
    summary="R√©cup√©rer les cat√©gories de pr√©f√©rences d'un utilisateur",
    tags=["Users"]
)
async def get_user_preference_categories(user_id: uuid.UUID, db: Session = Depends(get_db)):
    """
    R√©cup√®re une liste unique de toutes les cat√©gories associ√©es aux pr√©f√©rences
    d'un utilisateur donn√©.

    - **user_id**: L'identifiant UUID de l'utilisateur.
    - **Retourne**: Une liste d'objets cat√©gorie.
    """
    # V√©rifier si l'utilisateur existe (bonne pratique)
    user = await db.get(models.UserModel, user_id)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"User with id {user_id} not found"
        )

    # Construction de la requ√™te avec SQLAlchemy
    # C'est la partie la plus importante.
    # On s√©lectionne les cat√©gories (models.CategorieModel)
    # On fait la jointure avec la table preference_categories
    # Puis la jointure avec la table des pr√©f√©rences (models.PreferenceModel)
    # On filtre pour ne garder que les pr√©f√©rences de l'utilisateur concern√©
    # On utilise distinct() pour s'assurer que chaque cat√©gorie n'appara√Æt qu'une seule fois
    query = (
        select(models.CategorieModel)
        .join(models.preference_categories_table)
        .join(models.PreferenceModel)
        .where(models.PreferenceModel.user_id == user_id)
        .distinct()
    )

    # Ex√©cution de la requ√™te et r√©cup√©ration des r√©sultats
    # .scalars().all() retourne une liste d'objets CategorieModel
    result = await db.execute(query)
    result_categories = result.scalars().all()

    return result_categories


@app.get(
    "/users/preferences/recommendations/feed/v1/{user_id}",
    response_model=List[schemas2.CagnottePostFeedResponse],
    summary="G√©n√©rer un feed bas√© sur les pr√©f√©rences initiales de l'utilisateur",
    tags=["Recommendations"]
)
async def get_user_preferences_feed(
    user_id: uuid.UUID,
    total_recommendations: int = 15,
    db: AsyncSession = Depends(get_db)
):
    """
    G√©n√®re un feed de recommandations pour un utilisateur bas√© sur les cat√©gories
    qu'il a s√©lectionn√©es comme pr√©f√©rences. C'est la strat√©gie de d√©marrage
    avant que le profil utilisateur (Redis) ne soit construit.
    La r√©cup√©ration des donn√©es est ultra-optimis√©e avec une seule requ√™te √† la BDD.
    """
    # =================================================================
    # √âTAPE 1: OBTENIR LES CAT√âGORIES DE PR√âF√âRENCES DE L'UTILISATEUR
    # =================================================================
    
    # On r√©cup√®re les cat√©gories de pr√©f√©rences directement depuis la base de donn√©es
    stmt_categories = (
        select(models.CategorieModel)
        .join(models.preference_categories_table)
        .join(models.PreferenceModel)
        .where(models.PreferenceModel.user_id == user_id)
        .distinct()
    )
    result_categories = await db.execute(stmt_categories)
    preference_categories = result_categories.scalars().all()

    if not preference_categories:
        # Si l'utilisateur n'a pas de pr√©f√©rences, on ne peut rien recommander
        return []

    # =================================================================
    # √âTAPE 2: CR√âER LE PLAN DE RECOMMANDATION
    # =================================================================
    
    # Contrairement au profil Redis, les pr√©f√©rences n'ont pas de score.
    # Nous allons donc distribuer les recommandations de mani√®re √©quitable.
    recommendations_plan = []
    num_categories = len(preference_categories)
    
    base_count = total_recommendations // num_categories
    remainder = total_recommendations % num_categories

    for i, category in enumerate(preference_categories):
        num_to_fetch = base_count
        if i < remainder:
            num_to_fetch += 1  # Distribuer le reste sur les premi√®res cat√©gories
        
        if num_to_fetch > 0:
            recommendations_plan.append((str(category.id), num_to_fetch))

    if not recommendations_plan:
        return []

    # =================================================================
    # √âTAPE 3: LA REQU√äTE UNIQUE ET OPTIMIS√âE (LOGIQUE IDENTIQUE)
    # =================================================================
    
    # --- CTE 1: S√©lection des cagnottes recommand√©es ---
    union_queries = []
    for category_id, num_to_fetch in recommendations_plan:
        if num_to_fetch > 0:
            sq = select(models.CagnotteModel.id).where(models.CagnotteModel.id_categorie == category_id).order_by(func.random()).limit(num_to_fetch)
            union_queries.append(sq)

    if not union_queries:
        return []

    recommended_cagnottes_cte = union_all(*union_queries).cte("recommended_cagnottes")

    # --- CTE 2: Identification du post le plus r√©cent pour chaque cagnotte ---
    Post = models.CagnottePostModel
    post_subquery = (
        select(
            Post.id,
            Post.id_cagnotte,
            func.row_number().over(
                partition_by=Post.id_cagnotte,
                order_by=Post.created_date.desc()
            ).label("rn")
        )
        .where(Post.id_cagnotte.in_(select(recommended_cagnottes_cte.c.id)))
        .subquery("ranked_posts")
    )
    latest_posts_cte = select(post_subquery.c.id, post_subquery.c.id_cagnotte).where(post_subquery.c.rn == 1).cte("latest_posts")

    # --- Requ√™te finale: Assemblage de tout ---
    Ressource = models.RessourceModel
    final_stmt = (
        select(Post, Ressource)
        .join(latest_posts_cte, Post.id == latest_posts_cte.c.id)
        .outerjoin(Ressource, Ressource.reference == Post.id)
        .options(
            selectinload(Post.cagnotte).selectinload(models.CagnotteModel.categorie),
            selectinload(Post.cagnotte).selectinload(models.CagnotteModel.admin),
            selectinload(Post.cagnotte).selectinload(models.CagnotteModel.sponsors).selectinload(models.SponsorModel.organisation),
            selectinload(Post.author)
        )
        .order_by(latest_posts_cte.c.id_cagnotte, Ressource.order_index.asc())
    )

    result = await db.execute(final_stmt)
    
    # =================================================================
    # √âTAPE 4: ASSEMBLAGE EN PYTHON (LOGIQUE IDENTIQUE)
    # =================================================================
    
    posts_with_resources = defaultdict(lambda: {"post": None, "resources": []})
    for post, resource in result.all():
        if not posts_with_resources[post.id]["post"]:
            posts_with_resources[post.id]["post"] = post
        if resource:
            posts_with_resources[post.id]["resources"].append(resource)

    final_response = []
    for item in posts_with_resources.values():
        feed_item = build_feed_response(item["post"], item["resources"])
        final_response.append(feed_item)
    
    # # ===================================================================
    # # √âTAPE 4: ENRICHISSEMENT DES URLS PR√âSIGN√âES
    # # ===================================================================
    # try:
    #     enriched_feed = await enrich_feed_with_presigned_urls(final_response)
        
    #     print(f"üé¨ Feed enrichi pour les pr√©f√©rences de l'utilisateur {user_id}: {len(enriched_feed)} √©l√©ments")        
    #     return enriched_feed
    
    # except Exception as e:
    #     print(f"‚ùå Erreur lors de l'enrichissement des URLs: {e}")
        
    #     # FALLBACK: Retourner les donn√©es sans URLs pr√©sign√©es
    #     # plut√¥t que de tout faire √©chouer
    #     print(f"‚ö†Ô∏è Fallback: retour des donn√©es sans URLs pr√©sign√©es")
        
    #     # Retourner les donn√©es brutes (sans enrichissement)
    #     # Attention: v√©rifier que votre mod√®le accepte des URLs vides/None
    #     return enriched_feed
    
    random.shuffle(final_response)
    return final_response

#===============================Preferences==========================




#===============================RECOMMANDATIONS==============================
@app.get("/recommendations/feed/v1/", response_model=List[schemas2.CagnottePostFeedResponse])
async def get_recommendations_feed(
    phone: Optional[str] = None,
    session_id: Optional[str] = None,
    total_recommendations: int = 15,
    db: AsyncSession = Depends(get_db),
    redis_client: aioredis.Redis = Depends(get_redis_client)
):
    """
    G√©n√®re des recommandations de cagnottes de mani√®re ultra-optimis√©e
    avec une seule requ√™te √† la base de donn√©es.
    """
    # =================================================================
    # √âTAPE 1: OBTENIR LE PLAN DE RECOMMANDATION (INCHANG√â, CAR D√âJ√Ä TR√àS RAPIDE)
    # =================================================================
    if not phone and not session_id:
        raise HTTPException(status_code=400, detail="Un 'phone' ou un 'session_id' doit √™tre fourni.")
    if phone and session_id:
        raise HTTPException(status_code=400, detail="Fournissez 'phone' OU 'session_id', pas les deux.")
        
    redis_key = f"profile:user:{phone}" if phone else f"profile:session:{session_id}"
    user_profile = await redis_client.hgetall(redis_key)
    
    if not user_profile:
        return []

    sorted_categories = sorted(user_profile.items(), key=lambda item: float(item[1]), reverse=True)
    recommendations_plan = []
    remaining_recommendations = total_recommendations

    # Logique de distribution dynamique (inchang√©e)
    # ... (le code pour remplir recommendations_plan est identique et performant)
    for i, (category_id, _) in enumerate(sorted_categories):
        if i == 0: num_to_fetch = round(total_recommendations * 0.5)
        elif i == 1: num_to_fetch = round(total_recommendations * 0.3)
        elif i == 2: num_to_fetch = round(total_recommendations * 0.15)
        else: num_to_fetch = max(1, round(remaining_recommendations / (len(sorted_categories) - i)))
        num_to_fetch = min(num_to_fetch, remaining_recommendations)
        if num_to_fetch > 0:
            recommendations_plan.append((category_id, num_to_fetch))
            remaining_recommendations -= num_to_fetch
        if remaining_recommendations <= 0: break
    if remaining_recommendations > 0 and recommendations_plan:
        top_category_id, top_count = recommendations_plan[0]
        recommendations_plan[0] = (top_category_id, top_count + remaining_recommendations)

    if not recommendations_plan:
        return []

    # =================================================================
    # √âTAPE 2: LA REQU√äTE UNIQUE ET OPTIMIS√âE (LE GRAND CHANGEMENT)
    # =================================================================
    
    # --- CTE 1: S√©lection des cagnottes recommand√©es ---
    union_queries = []
    for category_id, num_to_fetch in recommendations_plan:
        if num_to_fetch > 0:
            sq = select(models.CagnotteModel.id).where(models.CagnotteModel.id_categorie == category_id).order_by(func.random()).limit(num_to_fetch)
            union_queries.append(sq)

    if not union_queries:
        return []

    recommended_cagnottes_cte = union_all(*union_queries).cte("recommended_cagnottes")

    # --- CTE 2: Identification du post le plus r√©cent pour chaque cagnotte ---
    Post = models.CagnottePostModel
    post_subquery = (
        select(
            Post.id,
            Post.id_cagnotte,
            func.row_number().over(
                partition_by=Post.id_cagnotte,
                order_by=Post.created_date.desc()
            ).label("rn")
        )
        .where(Post.id_cagnotte.in_(select(recommended_cagnottes_cte.c.id)))
        .subquery("ranked_posts")
    )
    latest_posts_cte = select(post_subquery.c.id, post_subquery.c.id_cagnotte).where(post_subquery.c.rn == 1).cte("latest_posts")

    # --- Requ√™te finale: Assemblage de tout ---
    Ressource = models.RessourceModel
    final_stmt = (
        select(Post, Ressource)
        .join(latest_posts_cte, Post.id == latest_posts_cte.c.id)
        # Utiliser `outerjoin` pour les ressources au cas o√π un post n'en aurait aucune
        .outerjoin(Ressource, Ressource.reference == Post.id)
        .options(
            # Charger les relations du Post en une seule fois pour √©viter des requ√™tes N+1
            selectinload(Post.cagnotte).selectinload(models.CagnotteModel.categorie),
            selectinload(Post.cagnotte).selectinload(models.CagnotteModel.sponsors).selectinload(models.SponsorModel.organisation),
            selectinload(Post.cagnotte).selectinload(models.CagnotteModel.admin),
            selectinload(Post.author)
        )
        .order_by(latest_posts_cte.c.id_cagnotte, Ressource.order_index.asc())
    )

    result = await db.execute(final_stmt)
     
    # =================================================================
    # √âTAPE 3: ASSEMBLAGE EN PYTHON (MAINTENANT PLUS SIMPLE ET RAPIDE)
    # =================================================================
    
    # Grouper les r√©sultats par post
    posts_with_resources = defaultdict(lambda: {"post": None, "resources": []})
    for post, resource in result.all():
        if not posts_with_resources[post.id]["post"]:
            posts_with_resources[post.id]["post"] = post
        if resource:
            posts_with_resources[post.id]["resources"].append(resource)

    # Construire la r√©ponse finale
    final_response = []
    for item in posts_with_resources.values():
        feed_item = build_feed_response(item["post"], item["resources"])
        final_response.append(feed_item)
    
    random.shuffle(final_response)
    return final_response

#================================RECOMMANDATIONS===============================



#=============================CAGNOTTE_MOMENT_BY_USER============================
@app.get(
    "/cagnottes/personalized/",
    response_model=List[schemas2.CagnottePostFeedResponse],
    summary="G√©n√®re des recommandations de posts personnalis√©es",
    description="Retourne les N meilleurs posts de la cat√©gorie pr√©f√©r√©e de l'utilisateur."
)
async def get_personalized_recommendations(
    db: AsyncSession = Depends(get_db),
    redis_client: aioredis.Redis = Depends(get_redis_client),
    phone: Optional[str] = Query(None, description="ID de l'utilisateur authentifi√©"),
    session_id: Optional[str] = Header(None, description="ID de session pour les utilisateurs anonymes"),
    limit: int = Query(5, ge=1, le=20, description="Nombre de posts √† recommander.")
):
    """
    Cet endpoint fournit des recommandations personnalis√©es bas√©es sur le profil
    d'engagement de l'utilisateur stock√© dans Redis.
    """
    # ===================================================================
    # √âTAPE 1: IDENTIFIER L'UTILISATEUR ET CONSTRUIRE LA CL√â REDIS
    # ===================================================================
    if not phone and not session_id:
        raise HTTPException(
            status_code=400,
            detail="Un 'phone' ou un 'session_id' doit √™tre fourni."
        )

    redis_key = f"profile:user:{phone}" if phone else f"profile:session:{session_id}"

    # ===================================================================
    # √âTAPE 2: R√âCUP√âRER LE PROFIL ET TROUVER LA CAT√âGORIE PR√âF√âR√âE
    # ===================================================================
    try:
        user_profile_raw = await redis_client.hgetall(redis_key)
        if not user_profile_raw:
            return []

        user_profile = { k: float(v) for k, v in user_profile_raw.items() }
        
        if not user_profile:
             return []

        top_category_id = max(user_profile, key=user_profile.get)

    except redis.RedisError as e:
        # logger.error(...)
        raise HTTPException(status_code=503, detail=f"Erreur de connexion √† Redis: {e}")
    except (ValueError, TypeError):
        return []

    # ===================================================================
    # √âTAPE 3: R√âCUP√âRER LES MEILLEURS POSTS DE CETTE CAT√âGORIE
    # ===================================================================
    Post = models.CagnottePostModel
    Cagnotte = models.CagnotteModel
    Ressource = models.RessourceModel

    # D√©finition de "meilleur post" : le plus de likes, puis le plus de vues
    stmt_posts = (
        select(Post)
        .join(Post.cagnotte)
        .options(
            selectinload(Post.cagnotte).selectinload(Cagnotte.categorie),
            selectinload(Post.author)
        )
        .where(Cagnotte.id_categorie == top_category_id)
        .order_by(Post.likes_count.desc(), Post.views_count.desc())
        .limit(limit)
    )

    result_posts = await db.execute(stmt_posts)
    top_posts = result_posts.scalars().unique().all()

    if not top_posts:
        return []

    # ===================================================================
    # √âTAPE 4: ENRICHIR AVEC LES RESSOURCES ET FORMATER LA R√âPONSE
    # ===================================================================
    post_ids = [p.id for p in top_posts]
    
    stmt_resources = (
        select(Ressource)
        .where(Ressource.reference.in_(post_ids))
        .order_by(Ressource.order_index.asc())
    )
    result_resources = await db.execute(stmt_resources)
    resources_by_post_id = defaultdict(list)
    for res in result_resources.scalars().all():
        resources_by_post_id[res.reference].append(res)
    
    # Assemblage de la r√©ponse finale
    final_response = []
    for post in top_posts:
        ressources = resources_by_post_id.get(post.id, [])
        feed_item = build_feed_response(post, ressources)
        final_response.append(feed_item)
        
    return final_response

#=============================CAGNOTTE_MOMENT_BY_USER============================


#=========================POPULARITER==========================
@app.get("/countries/popularity")
async def get_countries_popularity(
    redis: aioredis.Redis = Depends(get_redis_client),
    limit: Optional[int] = Query(None, ge=1, le=100, description="Nombre maximum de pays √† retourner"),
    sort_by: str = Query("popularity_score", description="Crit√®re de tri: popularity_score, unique_users, total_events"),
    order: str = Query("desc", description="Ordre: asc ou desc")
):
    """
    R√©cup√®re la popularit√© par pays depuis Redis
    """
    try:
        # R√©cup√©rer les donn√©es depuis Redis
        data = await redis.get("analytics_popularity_by_country")
        if not data:
            raise HTTPException(status_code=404, detail="Donn√©es de popularit√© par pays non trouv√©es")
        
        countries_data = json.loads(data)
        
        # Convertir en liste pour le tri
        countries_list = list(countries_data.values())
        
        # Tri
        reverse = order.lower() == "desc"
        try:
            countries_list.sort(key=lambda x: x.get(sort_by, 0), reverse=reverse)
        except (KeyError, TypeError):
            raise HTTPException(status_code=400, detail=f"Crit√®re de tri '{sort_by}' invalide")
        
        # Limitation
        if limit:
            countries_list = countries_list[:limit]
        
        return {
            "status": "success",
            "data": countries_list,
            "total_countries": len(countries_data),
            "returned_countries": len(countries_list),
            "sorted_by": sort_by,
            "order": order,
            "timestamp": datetime.now().isoformat()
        }
        
    except json.JSONDecodeError:
        raise HTTPException(status_code=500, detail="Erreur de d√©codage des donn√©es Redis")
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des donn√©es pays: {e}")
        raise HTTPException(status_code=500, detail="Erreur interne du serveur")


@app.get("/countries/popularity/{country}")
async def get_country_popularity(
    country: str,
    redis: aioredis.Redis = Depends(get_redis_client)
):
    """
    R√©cup√®re les d√©tails de popularit√© pour un pays sp√©cifique
    """
    try:
        data = await redis.get("analytics_popularity_by_country")
        if not data:
            raise HTTPException(status_code=404, detail="Donn√©es de popularit√© non trouv√©es")
        
        countries_data = json.loads(data)
        
        country_data = countries_data.get(country)
        if not country_data:
            raise HTTPException(status_code=404, detail=f"Pays '{country}' non trouv√©")
        
        return {
            "status": "success",
            "data": country_data,
            "timestamp": datetime.now().isoformat()
        }
        
    except json.JSONDecodeError:
        raise HTTPException(status_code=500, detail="Erreur de d√©codage des donn√©es Redis")
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des donn√©es pour {country}: {e}")
        raise HTTPException(status_code=500, detail="Erreur interne du serveur")

#=========================POPULARITER==========================


#==========================TRENDING==========================
@app.get("/trending/countries")
async def get_trending_countries(
    redis: aioredis.Redis = Depends(get_redis_client),
    trend_direction: Optional[str] = Query(None, description="Filtrer par direction: up, down, stable"),
    limit: Optional[int] = Query(None, ge=1, le=100, description="Nombre maximum de pays √† retourner")
):
    """
    R√©cup√®re les tendances par pays depuis Redis
    """
    try:
        data = await redis.get("analytics_trending_by_country")
        if not data:
            raise HTTPException(status_code=404, detail="Donn√©es de tendances par pays non trouv√©es")
        
        trending_data = json.loads(data)
        trending_list = list(trending_data.values())
        
        # Filtrage par direction de tendance
        if trend_direction:
            trending_list = [item for item in trending_list if item.get('trend_direction') == trend_direction]
        
        # Tri par trending_score
        trending_list.sort(key=lambda x: x.get('trending_score', 0), reverse=True)
        
        # Limitation
        if limit:
            trending_list = trending_list[:limit]
        
        return {
            "status": "success",
            "data": trending_list,
            "total_countries": len(trending_data),
            "returned_countries": len(trending_list),
            "filter_direction": trend_direction,
            "timestamp": datetime.now().isoformat()
        }
    
    except json.JSONDecodeError:
        raise HTTPException(status_code=500, detail="Erreur de d√©codage des donn√©es Redis")
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des tendances pays: {e}")
        raise HTTPException(status_code=500, detail="Erreur interne du serveur")

@app.get("/trending/categories")
async def get_trending_categories(
    redis: aioredis.Redis = Depends(get_redis_client),
    trend_direction: Optional[str] = Query(None, description="Filtrer par direction: up, down, stable"),
    limit: Optional[int] = Query(None, ge=1, le=100, description="Nombre maximum de cat√©gories √† retourner")
):
    """
    R√©cup√®re les tendances par cat√©gorie depuis Redis
    """
    try:
        data = await redis.get("analytics_trending_by_category")
        if not data:
            raise HTTPException(status_code=404, detail="Donn√©es de tendances par cat√©gorie non trouv√©es")
        
        trending_data = json.loads(data)
        trending_list = list(trending_data.values())
        
        # Filtrage par direction de tendance
        if trend_direction:
            trending_list = [item for item in trending_list if item.get('trend_direction') == trend_direction]
        
        # Tri par trending_score
        trending_list.sort(key=lambda x: x.get('trending_score', 0), reverse=True)
        
        # Limitation
        if limit:
            trending_list = trending_list[:limit]
        
        return {
            "status": "success",
            "data": trending_list,
            "total_categories": len(trending_data),
            "returned_categories": len(trending_list),
            "filter_direction": trend_direction,
            "timestamp": datetime.now().isoformat()
        }
    
    except json.JSONDecodeError:
        raise HTTPException(status_code=500, detail="Erreur de d√©codage des donn√©es Redis")
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des tendances cat√©gories: {e}")
        raise HTTPException(status_code=500, detail="Erreur interne du serveur")

# trending cold start recommendations videos "up" only
@app.get("/trending/up/{country}")
async def get_cold_start_recommendations(
    country: str,
    redis: aioredis.Redis = Depends(get_redis_client),
    min_velocity_percent: Optional[float] = Query(0.0, description="V√©locit√© minimale requise (%)"),
    min_trending_score: Optional[float] = Query(0.0, description="Score de tendance minimal"),
    exclude_stable_trends: Optional[bool] = Query(True, description="Exclure les tendances stables"),
    max_recommendations: Optional[int] = Query(10, ge=1, le=50, description="Nombre max de recommandations")
):
    """
    G√©n√®re des recommandations cold start pour un pays donn√©.
    Recommande uniquement les vid√©os avec une tendance croissante (velocity > 0 et direction 'up').
    """
    try:
        # R√©cup√©rer les donn√©es de tendance par pays
        data = await redis.get("analytics_trending_by_country")
        if not data:
            raise HTTPException(status_code=404, detail="Donn√©es de tendances non disponibles")
        
        trending_data = json.loads(data)
        
        if country not in trending_data:
            available_countries = list(trending_data.keys())
            raise HTTPException(
                status_code=404,
                detail=f"Pays '{country}' non trouv√©. Pays disponibles: {available_countries[:10]}"
            )
        
        country_data = trending_data[country]
        trending_videos = country_data.get('trending_videos', [])
        
        if not trending_videos:
            return {
                "status": "success",
                "data": {
                    "country": country,
                    "recommended_videos": [],
                    "country_context": country_data,
                },
                "total_videos": 0,
                "filtered_videos": 0,
                "filter_criteria": {
                    "min_velocity_percent": min_velocity_percent,
                    "min_trending_score": min_trending_score,
                    "exclude_stable_trends": exclude_stable_trends
                },
                "message": f"Aucune vid√©o en tendance trouv√©e pour le pays '{country}'",
                "timestamp": datetime.now().isoformat()
            }
        
        # Filtrer les vid√©os selon les crit√®res cold start
        filtered_videos = []
        for video in trending_videos:
            # Crit√®re 1: Tendance croissante uniquement (v√©locit√© positive)
            if video.get('velocity_percent', 0) <= min_velocity_percent:
                continue
            
            # Crit√®re 2: Direction de tendance "up" uniquement
            if video.get('trend_direction') != 'up':
                continue
            
            # Crit√®re 3: Score de tendance minimum
            if video.get('trending_score', 0) < min_trending_score:
                continue
            
            # Crit√®re 4: Exclure les tendances stables si demand√©
            if exclude_stable_trends and video.get('trend_direction') == 'stable':
                continue
            
            filtered_videos.append(video)
        
        # Trier par score de tendance d√©croissant et limiter
        filtered_videos.sort(key=lambda x: x.get('trending_score', 0), reverse=True)
        final_videos = filtered_videos[:max_recommendations]
        
        return {
            "status": "success",
            "data": {
                "country": country,
                "recommended_videos": final_videos,
                "country_context": {
                    "country_trending_score": country_data.get('trending_score', 0),
                    "country_velocity_percent": country_data.get('velocity_percent', 0),
                    "country_trend_direction": country_data.get('trend_direction', 'stable'),
                    "total_trending_videos": len(trending_videos),
                    "unique_users_recent": country_data.get('unique_users_recent', 0)
                }
            },
            "total_videos": len(trending_videos),
            "filtered_videos": len(final_videos),
            "filter_criteria": {
                "min_velocity_percent": min_velocity_percent,
                "min_trending_score": min_trending_score,
                "exclude_stable_trends": exclude_stable_trends,
                "max_recommendations": max_recommendations
            },
            "statistics": {
                "average_velocity": sum(v.get('velocity_percent', 0) for v in final_videos) / len(final_videos) if final_videos else 0,
                "average_trending_score": sum(v.get('trending_score', 0) for v in final_videos) / len(final_videos) if final_videos else 0
            },
            "timestamp": datetime.now().isoformat()
        }
        
    except json.JSONDecodeError:
        raise HTTPException(status_code=500, detail="Erreur de d√©codage des donn√©es Redis")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration des recommandations pour {country}: {e}")
        raise HTTPException(status_code=500, detail="Erreur interne du serveur")

@app.get("/trending/global")
async def get_global_trending_recommendations(
    redis: aioredis.Redis = Depends(get_redis_client),
    min_velocity_percent: Optional[float] = Query(5.0, description="V√©locit√© minimale requise (%)"),
    min_trending_score: Optional[float] = Query(1.0, description="Score de tendance minimal"),
    max_recommendations: Optional[int] = Query(20, ge=1, le=100, description="Nombre max de recommandations"),
    trend_direction: Optional[str] = Query("up", description="Direction de tendance: up, down, stable")
):
    """
    R√©cup√®re les vid√©os les plus en tendance globalement (tous pays confondus)
    avec filtrage par direction de tendance
    """
    try:
        data = await redis.get("analytics_trending_by_country")
        if not data:
            raise HTTPException(status_code=404, detail="Donn√©es de tendances non disponibles")
        
        trending_data = json.loads(data)
        
        # Collecter toutes les vid√©os en tendance de tous les pays
        all_videos = []
        video_scores = {}  # Pour d√©duplication
        
        for country, country_data in trending_data.items():
            trending_videos = country_data.get('trending_videos', [])
            
            for video in trending_videos:
                # Filtrer selon les crit√®res
                if (video.get('velocity_percent', 0) >= min_velocity_percent and 
                    video.get('trending_score', 0) >= min_trending_score):
                    
                    # Filtrer par direction si sp√©cifi√©e
                    if trend_direction and video.get('trend_direction') != trend_direction:
                        continue
                    
                    video_id = video['video_id']
                    # Garder le meilleur score si la vid√©o appara√Æt dans plusieurs pays
                    if (video_id not in video_scores or 
                        video['trending_score'] > video_scores[video_id]['trending_score']):
                        video_copy = video.copy()
                        video_copy['source_country'] = country
                        video_scores[video_id] = video_copy
        
        # Trier par score de tendance et limiter
        sorted_videos = sorted(video_scores.values(), 
                             key=lambda x: x['trending_score'], reverse=True)
        final_videos = sorted_videos[:max_recommendations]
        
        return {
            "status": "success",
            "data": final_videos,
            "total_unique_videos": len(video_scores),
            "returned_videos": len(final_videos),
            "filter_criteria": {
                "min_velocity_percent": min_velocity_percent,
                "min_trending_score": min_trending_score,
                "trend_direction": trend_direction,
                "max_recommendations": max_recommendations
            },
            "statistics": {
                "countries_analyzed": len(trending_data),
                "average_velocity": sum(v.get('velocity_percent', 0) for v in final_videos) / len(final_videos) if final_videos else 0,
                "average_trending_score": sum(v.get('trending_score', 0) for v in final_videos) / len(final_videos) if final_videos else 0
            },
            "timestamp": datetime.now().isoformat()
        }
        
    except json.JSONDecodeError:
        raise HTTPException(status_code=500, detail="Erreur de d√©codage des donn√©es Redis")
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des recommandations globales: {e}")
        raise HTTPException(status_code=500, detail="Erreur interne du serveur")
#==========================TRENDING==========================


#=========================CATEGORIES==========================
#Liste des cat√©gories par popularit√©
@app.get("/categories/popularity")
async def get_categories_popularity(
    redis: aioredis.Redis = Depends(get_redis_client),
    limit: Optional[int] = Query(None, ge=1, le=100, description="Nombre maximum de cat√©gories √† retourner"),
    sort_by: str = Query("popularity_score", description="Crit√®re de tri"),
    order: str = Query("desc", description="Ordre: asc ou desc")
):
    """
    R√©cup√®re la popularit√© par cat√©gorie depuis Redis
    """
    try:
        data = await redis.get("analytics_popularity_by_category")
        if not data:
            raise HTTPException(status_code=404, detail="Donn√©es de popularit√© par cat√©gorie non trouv√©es")
        
        categories_data = json.loads(data)
        categories_list = list(categories_data.values())
        
        # Tri
        reverse = order.lower() == "desc"
        try:
            categories_list.sort(key=lambda x: x.get(sort_by, 0), reverse=reverse)
        except (KeyError, TypeError):
            raise HTTPException(status_code=400, detail=f"Crit√®re de tri '{sort_by}' invalide")
        
        # Limitation
        if limit:
            categories_list = categories_list[:limit]
        
        return {
            "status": "success",
            "data": categories_list,
            "total_categories": len(categories_data),
            "returned_categories": len(categories_list),
            "sorted_by": sort_by,
            "order": order,
            "timestamp": datetime.now().isoformat()
        }
        
    except json.JSONDecodeError:
        raise HTTPException(status_code=500, detail="Erreur de d√©codage des donn√©es Redis")
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des donn√©es cat√©gories: {e}")
        raise HTTPException(status_code=500, detail="Erreur interne du serveur")


#√©tails pour une cat√©gorie sp√©cifique
@app.get("/categories/popularity/{category_id}")
async def get_category_popularity(
    category_id: str,
    redis: aioredis.Redis = Depends(get_redis_client)
):
    """
    R√©cup√®re les d√©tails de popularit√© pour une cat√©gorie sp√©cifique
    """
    try:
        data = await redis.get("analytics_popularity_by_category")
        if not data:
            raise HTTPException(status_code=404, detail="Donn√©es de popularit√© non trouv√©es")
        
        categories_data = json.loads(data)
        category_data = categories_data.get(category_id)
        if not category_data:
            raise HTTPException(status_code=404, detail=f"Cat√©gorie '{category_id}' non trouv√©e")
        
        return {
            "status": "success",
            "data": category_data,
            "timestamp": datetime.now().isoformat()
        }
        
    except json.JSONDecodeError:
        raise HTTPException(status_code=500, detail="Erreur de d√©codage des donn√©es Redis")
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des donn√©es pour la cat√©gorie {category_id}: {e}")
        raise HTTPException(status_code=500, detail="Erreur interne du serveur")

#=========================CATEGORIES==========================


# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Analytiques & Recommandations
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++